---
title: "Human versus artificial intelligence in oral pathology diagnosis: a comparative study of ChatGPT, Grok, and MANUS."
description: "Artificial intelligence (AI) integration in diagnostic medicine has advanced accuracy and efficiency, particularly in pathology. This study assessed the diagnostic performance of three large language models (LLMs)-ChatGPT (GPT-4-turbo), Grok (xAI), a..."
date: "2026-02-25"
category: "ia-odontologia"
pubmedId: "41735394"
author: "Abdullah F Alshammari, Ahmed A Madfa, Bassam Ali Anazi"
tags: ["Artificial intelligence", "Diagnostic accuracy", "Digital pathology", "Large language models", "Oral pathology"]
---

## Resumen

Artificial intelligence (AI) integration in diagnostic medicine has advanced accuracy and efficiency, particularly in pathology. This study assessed the diagnostic performance of three large language models (LLMs)-ChatGPT (GPT-4-turbo), Grok (xAI), and MANUS-in interpreting histopathology slides of oral lesions. A comparative diagnostic study was conducted using 100 high-resolution slides representing diverse oral pathologies. Images were sourced from a validated textbook and reviewed by two board-certified oral pathologists who provided consensus diagnoses. Each slide was analysed twice by the three AI models using standardized prompts. Diagnostic accuracy, intra-model consistency, inter-model concordance, and agreement with human experts were evaluated using descriptive statistics, Cohen's kappa, McNemar's test, and chi-square analysis. All AI models demonstrated high diagnostic accuracy. In the second round, Grok achieved the highest accuracy (97%), followed by MANUS (96%) and ChatGPT (94%). ChatGPT showed the highest intra-model consistency (&#x3ba;&#x2009;=&#x2009;0.918), while MANUS and Grok displayed substantial agreement (&#x3ba;&#x2009;=&#x2009;0.790 and 0.740). Expert pathologists achieved 98% accuracy. Comparisons between AI models and human diagnoses showed moderate to substantial agreement, with MANUS most aligned with experts. Most misclassifications occurred in histologically ambiguous cases, with no significant differences between AI models. Multimodal LLMs demonstrated strong diagnostic capabilities, consistency, and alignment with expert reasoning in oral histopathology interpretation. Grok was the most accurate, ChatGPT the most consistent, and MANUS the most expert-aligned. These findings support AI integration into digital pathology for diagnostic support, education, and quality assurance, with further validation in clinical datasets recommended.

## Información del artículo

- **Revista:** Scientific reports
- **Fecha de publicación:** 2026-02-25
- **Autores:** Abdullah F Alshammari, Ahmed A Madfa, Bassam Ali Anazi
- **DOI:** [10.1038/s41598-026-40792-0](https://doi.org/10.1038/s41598-026-40792-0)
- **PubMed ID:** [41735394](https://pubmed.ncbi.nlm.nih.gov/41735394/)

## Referencias

Este artículo fue obtenido automáticamente desde [PubMed](https://pubmed.ncbi.nlm.nih.gov/41735394/), la base de datos de literatura biomédica del National Center for Biotechnology Information (NCBI).
