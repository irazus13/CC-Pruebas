---
title: "The effectiveness of large language models in dental specialty questions: a comparative study in the field of prosthodontics."
description: "This study was aimed to compare the accuracy of four state-of-the-art large language models (LLMs)(ChatGPT-5, ChatGPT-4o, DeepSeek, and Gemini 2.5 Pro) on Dentistry Specialization Exam (DUS) questions in Prosthodontics, and to assess t..."
date: "2026-02-13"
category: "ia-odontologia"
pubmedId: "41689029"
author: "Selda G&#xf6;k&#xe7;e Erdal, Aynur Beyza &#xc7;avu&#x15f;culu G&#xfc;d&#xfc;l, Ay&#x15f;eg&#xfc;l K&#xf6;ro&#x11f;lu"
tags: ["Artificial intelligence", "ChatGPT", "Chatbots", "DeepSeek", "Gemini"]
---

## Resumen

**OBJECTIVE:** This study was aimed to compare the accuracy of four state-of-the-art large language models (LLMs)(ChatGPT-5, ChatGPT-4o, DeepSeek, and Gemini 2.5 Pro) on Dentistry Specialization Exam (DUS) questions in Prosthodontics, and to assess the effect of querying method on performance. **MATERIALS AND METHODS:** A total of 128 multiple-choice DUS questions (106 knowledge-based; 22 case-based) from 13 exams (2012-2021) were administered. Each model was tested within a single 24-hour window (September 9, 2025). Two protocols were used: (1) Independent Query (each item in a fresh chat) and (2) Sequential Query (exam-like blocks of 10 items). First responses only were recorded and scored against official answer keys. Statistical analyses were performed using the chi-square (&#x3c7;&#xb2;) test to compare categorical outcomes (correct/incorrect responses) across different large language models and query strategies. Inter-rater agreement for question classification was assessed using Cohen's kappa coefficient, and all analyses were conducted at a significance level of &#x3b1; = 0.05. **RESULTS:** With Independent Query, the accuracy for knowledge questions was found to be 91% (GPT-5), 86% (GPT-4o), 71% (DeepSeek), and 88% (Gemini 2.5 Pro) while total accuracy was detected as 86%, 83%, 66%, and 85% respectively. In this context, statistically significant differences were reported across models, also due to the low scores of DeepSeek. Case-based accuracies (64%, 68%, 45%, and 77%) did not differ significantly. With Sequential Query, knowledge accuracies were 75%, 73%, 63%, and 82% and case accuracies were 77%, 68%, 64%, and 91%, respectively. Total accuracy still differed across models (75%, 72%, 63%, and 84%). Within-model comparisons showed significant drops for knowledge items from Independent to Sequential querying for GPT-5 (91%&#x2192;75%) and GPT-4o (86%&#x2192;73%). DeepSeek and Gemini 2.5 Pro showed no significant changes. Notably, Gemini 2.5 Pro yielded the highest performance in case-based questions (91%) with sequential query. **CONCLUSIONS:** While the present study findings highlight current limitations in clinical reasoning, they support the conclusion that LLMs can be used as supplementary educational tools for DUS-style knowledge assessment but should not replace expert judgment or patient-specific decision-making.

## Información del artículo

- **Revista:** BMC medical education
- **Fecha de publicación:** 2026-02-13
- **Autores:** Selda G&#xf6;k&#xe7;e Erdal, Aynur Beyza &#xc7;avu&#x15f;culu G&#xfc;d&#xfc;l, Ay&#x15f;eg&#xfc;l K&#xf6;ro&#x11f;lu
- **DOI:** [10.1186/s12909-026-08808-5](https://doi.org/10.1186/s12909-026-08808-5)
- **PubMed ID:** [41689029](https://pubmed.ncbi.nlm.nih.gov/41689029/)

## Referencias

Este artículo fue obtenido automáticamente desde [PubMed](https://pubmed.ncbi.nlm.nih.gov/41689029/), la base de datos de literatura biomédica del National Center for Biotechnology Information (NCBI).
