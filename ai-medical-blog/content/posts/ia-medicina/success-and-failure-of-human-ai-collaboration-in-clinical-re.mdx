---
title: "Success and failure of human-AI collaboration in clinical reasoning: An experimental study on challenging real-world cases."
description: "While conversational human-AI collaboration (HAC) using large language models (LLM) has shown potential to enhance clinical reasoning, its effectiveness in highly specialized and challenging clinical scenarios remains unclear. This st..."
date: "2026-02-10"
category: "ia-medicina"
pubmedId: "41689881"
author: "Kai Tzu-Iunn Ong, Junwon Seo, Hyojun Kim et al."
tags: ["Clinical reasoning", "Cognitive burden", "Confidence", "Large language model", "Model behaviors"]
---

## Resumen

**BACKGROUND:** While conversational human-AI collaboration (HAC) using large language models (LLM) has shown potential to enhance clinical reasoning, its effectiveness in highly specialized and challenging clinical scenarios remains unclear. This study aimed to evaluate the effectiveness of HAC and analyzed the causes of its success and failure. **METHODS:** A crossover experimental study was conducted using 30 challenging cases from JAMA Ophthalmology. Thirty participants (10 board-certified ophthalmologist, 10 ophthalmology resident, and 10 senior medical students) completed the cases under two conditions: independent work (human-only) and collaboration through free-text conversation with Claude-3.5-Sonnet (HAC). Performance accuracy, along with self-rated confidence and cognitive burden, were assessed. HAC interaction logs were analyzed to evaluate the appropriateness of the LLM's accepting and arguing behaviors, which were categorized into six patterns. Sliding paired t-tests across incremental thresholds were used to assess how accuracy gains from HAC varied by task difficulty. **RESULTS:** HAC significantly improved mean accuracy compared to the human-only condition (from 0.45 to 0.60, P &lt; 0.001), although 20% of participants showed a decline in performance and the mean remained below the LLM-only accuracy (0.70). HAC significantly increased confidence and reduced cognitive burden (both P &lt; 0.001) in both successful and failed HAC. The appropriateness of LLM behaviors was substantially higher in successful HAC than in failed HAC (F1 score&#xa0;=&#xa0;0.92 vs. 0.29, P &lt; 0.001). In successful HAC, 92.6% followed the pattern LLM presents correct insight/human accepts, while 58.6% of failures involved LLM presents incorrect insight/human accepts. HAC improved accuracy significantly in tasks where the human-only correct response rate exceeded 47% (P &lt; 0.05), but not below 30% (P &#x2265; 0.188). **CONCLUSIONS:** These findings suggest that HAC benefits complex clinical decisions in ophthalmology but remains limited by human, model, and task-level factors requiring further improvement.

## Información del artículo

- **Revista:** International journal of medical informatics
- **Fecha de publicación:** 2026-02-10
- **Autores:** Kai Tzu-Iunn Ong, Junwon Seo, Hyojun Kim, Jiwoo Kim, Jihoon Kim
- **DOI:** [10.1016/j.ijmedinf.2026.106342](https://doi.org/10.1016/j.ijmedinf.2026.106342)
- **PubMed ID:** [41689881](https://pubmed.ncbi.nlm.nih.gov/41689881/)

## Referencias

Este artículo fue obtenido automáticamente desde [PubMed](https://pubmed.ncbi.nlm.nih.gov/41689881/), la base de datos de literatura biomédica del National Center for Biotechnology Information (NCBI).
