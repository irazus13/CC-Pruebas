---
title: "Fine-Tuned Large Language Models for Generating Multiple-Choice Questions in Anesthesiology: Psychometric Comparison With Faculty-Written Items."
description: "Multiple-choice examinations (MCQs) are widely used in medical education to ensure standardized and objective assessment. Developing high-quality items requires both subject expertise and methodological rigor. Large language models (L..."
date: "2026-02-18"
category: "ia-medicina"
pubmedId: "41707182"
author: "Carlos Ramon H&#xf6;lzing, Charlotte Meynhardt, Patrick Meybohm et al."
tags: ["anesthesiology", "artificial intelligence", "assessment", "fine-tuning", "large language models"]
---

## Resumen

**BACKGROUND:** Multiple-choice examinations (MCQs) are widely used in medical education to ensure standardized and objective assessment. Developing high-quality items requires both subject expertise and methodological rigor. Large language models (LLMs) offer new opportunities for automated item generation. However, most evaluations rely on general-purpose prompting, and psychometric comparisons with faculty-written items remain scarce. **OBJECTIVE:** This study aimed to evaluate whether a fine-tuned LLM can generate MCQs (Type A) in anesthesiology with psychometric properties comparable to those written by expert faculty. **METHODS:** The study was embedded in the regular written anesthesiology examination of the eighth-semester medical curriculum with 157 students. The examination comprised 30 single best-answer MCQs, of which 15 were generated by senior faculty and 15 by a fine-tuned GPT-based model. A custom GPT-based (GPT-4) model was adapted with anesthesiology lecture slides, the National Competence-Based Learning Objectives Catalogue (NKLM 2.0), past examination questions, and faculty publications using supervised instruction-tuning with standardized prompt-response pairs. Item analysis followed established psychometric standards. **RESULTS:** In total, 29 items (14 expert, 15 LLM-generated) were analyzed. Expert-generated questions had a mean difficulty of 0.81 (SD 0.19), point-biserial correlation of 0.19 (SD 0.07), and discrimination index of 0.09 (SD 0.08). LLM-generated items had a mean difficulty of 0.79 (SD 0.18), point-biserial correlation of 0.17 (SD 0.04), and discrimination index of 0.08 (SD 0.11). Mann-Whitney U tests revealed no significant differences between expert- and LLM-generated items for difficulty (P=.38), point-biserial correlation coefficient (P=.96), or discrimination index (P=.59). Categorical analyses confirmed no significant group differences. Both sets, however, showed only modest psychometric quality. **CONCLUSIONS:** Supervised fine-tuned LLMs are capable of generating MCQs with psychometric properties comparable to those written by experienced faculty. Given the limitations and cohort-dependency of psychometric indices, automated item generation should be considered a complement rather than a replacement for manual item writing. Further research with larger item sets and multi-institutional validation is needed to confirm generalizability and optimize integration of LLM-based tools into assessment development.

## Información del artículo

- **Revista:** JMIR formative research
- **Fecha de publicación:** 2026-02-18
- **Autores:** Carlos Ramon H&#xf6;lzing, Charlotte Meynhardt, Patrick Meybohm, Sarah K&#xf6;nig, Peter Kranke
- **DOI:** [10.2196/84904](https://doi.org/10.2196/84904)
- **PubMed ID:** [41707182](https://pubmed.ncbi.nlm.nih.gov/41707182/)

## Referencias

Este artículo fue obtenido automáticamente desde [PubMed](https://pubmed.ncbi.nlm.nih.gov/41707182/), la base de datos de literatura biomédica del National Center for Biotechnology Information (NCBI).
